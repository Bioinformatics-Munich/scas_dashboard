@misc{grafanadb,
  author = {Matteo Dessalvi},
  title = {{SLURM Dashboard}},
  howpublished = "\url{https://grafana.com/grafana/dashboards/4323}",
  year = {2021}, 
  note = "[Online; accessed 30-August-2023]"
}

@Manual{R,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2023},
  url = {https://www.R-project.org/},
}

@Manual{shiny,
  title = {shiny: Web Application Framework for R},
  author = {Winston Chang and Joe Cheng and JJ Allaire and Carson Sievert and Barret Schloerke and Yihui Xie and Jeff Allen and Jonathan McPherson and Alan Dipert and Barbara Borges},
  year = {2023},
  note = {https://shiny.posit.co/,
https://github.com/rstudio/shiny},
}

@Manual{shinydashboard,
  title = {shinydashboard: Create Dashboards with 'Shiny'},
  author = {Winston Chang and Barbara {Borges Ribeiro}},
  year = {2021},
  note = {R package version 0.7.2},
  url = {http://rstudio.github.io/shinydashboard/},
}

@InProceedings{slurm,
author="Yoo, Andy B.
and Jette, Morris A.
and Grondona, Mark",
editor="Feitelson, Dror
and Rudolph, Larry
and Schwiegelshohn, Uwe",
title="SLURM: Simple Linux Utility for Resource Management",
booktitle="Job Scheduling Strategies for Parallel Processing",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="44--60",
abstract="A new cluster resource management system called Simple Linux Utility Resource Management (SLURM) is described in this paper. SLURM, initially developed for large Linux clusters at the Lawrence Livermore National Laboratory (LLNL), is a simple cluster manager that can scale to thousands of processors. SLURM is designed to be flexible and fault-tolerant and can be ported to other clusters of different size and architecture with minimal effort. We are certain that SLURM will benefit both users and system architects by providing them with a simple, robust, and highly scalable parallel job execution environment for their cluster system.",
isbn="978-3-540-39727-4",
doi = {10.1007/10968987_3}
}

@ARTICLE{xdmod,
author={Palmer, Jeffrey T. and Gallo, Steven M. and Furlani, Thomas R. and Jones, Matthew D. and DeLeon, Robert L. and White, Joseph P. and Simakov, Nikolay and Patra, Abani K. and Sperhac, Jeanette and Yearke, Thomas and Rathsam, Ryan and Innus, Martins and Cornelius, Cynthia D. and Browne, James C. and Barth, William L. and Evans, Richard T.},
journal={Computing in Science & Engineering}, 
title={Open XDMoD: A Tool for the Comprehensive Management of High-Performance Computing Resources}, 
year={2015},
volume={17},
number={4},
pages={52-62},
keywords={Data warehouses;Measurement;Open source software;Open source hardware;Quality of service;Open XDMoD;XDMoD;TACC_Stats;SUPReMM;high-performance computing;HPC;HPC resource management;HPC metrics;application kernels;scientific computing},
doi={10.1109/MCSE.2015.68}}


@article{Hudak2018, 
doi = {10.21105/joss.00622}, 
url = {https://doi.org/10.21105/joss.00622}, 
year = {2018}, 
publisher = {The Open Journal}, 
volume = {3}, 
number = {25}, 
pages = {622}, 
author = {Dave Hudak and Doug Johnson and Alan Chalker and Jeremy Nicklas and Eric Franz and Trey Dockendorf and Brian L. McMichael}, 
title = {Open OnDemand: A web-based client portal for HPC centers}, 
journal = {Journal of Open Source Software} 
}
